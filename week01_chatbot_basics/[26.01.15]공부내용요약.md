📅 Day 2(26/01/15) 학습 요약 — LLM/챗봇 기초 구조
1️⃣ 개발 환경 상태

Conda 가상환경 ai-study 생성 및 활성화

Python 3.10.19 사용

VSCode + Jupyter 커널 정상 연결

GPU 인식 확인

torch.cuda.is_available() == True

GPU: RTX 4060 Laptop

2️⃣ Week 1 챗봇 실습 목표

“대답하는 코드” ❌

“상태(State)를 가진 챗봇 구조 이해” ⭕

3️⃣ 핵심 개념 정리
🔹 ChatState 클래스
class ChatState:
    def __init__(self):
        self.user_name = None
        self.step = "ask_name"


챗봇의 기억을 담는 객체

로직 ❌ / 저장 ❌

현재 대화 상태만 관리

🔹 state = ChatState()

클래스 생성(create)

copy ❌

새로운 메모리 공간에 객체 생성

🔹 상태 전달 방식

전역 변수 ❌

함수 인자로 state 전달 ⭕

LLM/RAG 확장 가능한 구조

4️⃣ 디버깅 미션에서 배운 것
❌ 버그 원인
chatbot(state, user_input)


함수 인자 순서 오류

str을 state로 전달 → 런타임 에러

✅ 해결
chatbot(user_input, state)

배운 교훈

파이썬은 타입 힌트가 있어도 런타임에서 막아주지 않음

키워드 인자 사용이 실무에서 안전

5️⃣ 얕은 복사 / 참조 개념

state2 = state1 → 참조 공유 (위험)

얕은 복사(shallow copy)는

내부에 리스트/딕셔너리 있을 때 치명적

챗봇 세션은 무조건 새 객체 생성

6️⃣ state 저장 위치에 대한 이해
현재 단계 (정답)

Python 메모리

왜?

빠름

단순함

학습/단일 사용자 환경에 적합

7️⃣ RAG 구조에 대한 사고 정리
데이터 구분

원본 지식 → 외부 저장소 (벡터 DB)

검색 결과 → 일시적 (state or 로컬 변수)

대화 맥락 → state

8️⃣ 캐싱에 대한 결론

RAG 검색 결과는 기본적으로 저장하지 않음

예외: 동일 질문 반복 시 캐싱 가능

캐시는 state ❌ / 전역(메모리·Redis) ⭕

“지식은 쌓고, 검색은 매번 새로 한다”

🧠 오늘의 핵심 한 문장

“챗봇은 기억하지 않는다.
상태 객체가 기억하고, 로직은 그걸 처리할 뿐이다.”